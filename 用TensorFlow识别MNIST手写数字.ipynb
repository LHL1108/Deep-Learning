{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"使用feed dictionary训练和评估MINIST网络\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "\n",
    "# 作为 external flags 的基本模型参数.\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def placeholder_inputs(batch_size):\n",
    "  \"\"\"生成占位符变量以代表张量 \n",
    "  \n",
    "    这些占位符将被用作剩余模型构建代码的输入，并且会被喂入后面.run()循环中的下载数据\n",
    "\n",
    "\n",
    "  参数:\n",
    "    batch_size: 批处理规模将同时be baked into两个占位符\n",
    "\n",
    "  返回值:\n",
    "    images_placeholder: 图片占位符.\n",
    "    labels_placeholder: 标签占位符.\n",
    "  \"\"\"\n",
    "  # 请注意占位符的形状与完整的图片和标签形状相匹配\n",
    "  # 但第一维现在是批处理规模而不是完整的训练和测试数据集的规模 \n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                         mnist.IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return images_placeholder, labels_placeholder\n",
    "\n",
    "\n",
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "  \"\"\"填充feed_dict以训练给定步骤\n",
    "\n",
    " 一个feed_dict的格式为:\n",
    "  feed_dict = {\n",
    "      <占位符>: <传递给占位符的值的张量>\n",
    "      ....\n",
    "  }\n",
    "\n",
    "  参数:\n",
    "    data_set: 从input_data.read_data_sets()中获取的图片和标签的集合\n",
    "    images_pl: 从placeholder_inputs()中获得的图片占位符\n",
    "    labels_pl: 从placeholder_inputs()中获得的标签占位符\n",
    "\n",
    "  返回值:\n",
    "    feed_dict: 从占位符到值得feed字典映射\n",
    "  \"\"\"\n",
    "  # 为填充了下一个批次规模示例的占位符创建feed_dict\n",
    "  images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,\n",
    "                                                 FLAGS.fake_data)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict\n",
    "\n",
    "\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  \"\"\"针对完整的一个周期运行一次评估。\n",
    "\n",
    "  参数:\n",
    "    sess: 关于哪个模型被训练的session\n",
    "    eval_correct: The Tensor返回正确的预测的数量\n",
    "    images_placeholder: 图片占位符\n",
    "    labels_placeholder: 标签占位符\n",
    "    data_set: 从input_data.read_data_sets()中获得的要评估的图像和标签集\n",
    "  \"\"\"\n",
    "  # 运行一个评估周期\n",
    "  true_count = 0  # 计算正确预测的数量\n",
    "  steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "  num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = float(true_count) / num_examples\n",
    "  print('Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n",
    "\n",
    "\n",
    "def run_training():\n",
    "  \"\"\"训练几步MNIST\"\"\"\n",
    "  # 在MNIST上获取图片和标签的训练集验证集和测试集Get the sets of images and labels for training, validation, and\n",
    "  # test on MNIST.\n",
    "  data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "  # 告诉TensorFlow,模型将构建进默认的图中\n",
    "  with tf.Graph().as_default():\n",
    "    # 为图片和标签产生占位符\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(\n",
    "        FLAGS.batch_size)\n",
    "\n",
    "    # 构建一个图，用于对模型计算预测\n",
    "    logits = mnist.inference(images_placeholder,\n",
    "                             FLAGS.hidden1,\n",
    "                             FLAGS.hidden2)\n",
    "\n",
    "    # 将损失的计算结果添加到图和节点当中去.\n",
    "    loss = mnist.loss(logits, labels_placeholder)\n",
    "\n",
    "    # 将计算和应用的梯度添加到图和节点当中去\n",
    "    train_op = mnist.training(loss, FLAGS.learning_rate)\n",
    "\n",
    "    # 添加OP以在评估期间比较logits和标签\n",
    "    eval_correct = mnist.evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # 将所有OP汇总为一个OP.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # 添加变量初始化OP\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # 创建一个编写检查点的存档.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # 创建一个在图上运行的节点的Session\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # 实例化SummaryWriter以输出摘要和图\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "    # 在所有都构建好以后:\n",
    "\n",
    "    # 运行OP来初始化变量\n",
    "    sess.run(init)\n",
    "\n",
    "    # 开始训练循环\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "\n",
    "      # 使用此特定训练步数中的图片和标签来填充feed字典\n",
    "      feed_dict = fill_feed_dict(data_sets.train,\n",
    "                                 images_placeholder,\n",
    "                                 labels_placeholder)\n",
    "\n",
    "      # 运行模型一步. 返回值是train_op(被丢弃)和loss op的激活\n",
    "      # 要想检查Ops和变量的值，你也许要将他们包含在一个sess.run()传递的列表中并且张量值会被返回到调用的元组中\n",
    "      # 且张量值会被返回到调用的元组中\n",
    "\n",
    "      _, loss_value = sess.run([train_op, loss],\n",
    "                               feed_dict=feed_dict)\n",
    "\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      # 编写摘要并经常打印概述\n",
    "      if step % 100 == 0:\n",
    "        # Print status to stdout.\n",
    "        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "        # Update the events file.\n",
    "        summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "      # 周期性的保存检查点并评估模型\n",
    "      if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_file, global_step=step)\n",
    "        # 通过训练集评估\n",
    "        print('Training Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.train)\n",
    "        # 通过验证集评估\n",
    "        print('Validation Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.validation)\n",
    "        # 通过测试集评估\n",
    "        print('Test Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.test)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "  run_training()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type=float,\n",
    "      default=0.01,\n",
    "      help='Initial learning rate.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--max_steps',\n",
    "      type=int,\n",
    "      default=2000,\n",
    "      help='Number of steps to run trainer.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--hidden1',\n",
    "      type=int,\n",
    "      default=128,\n",
    "      help='Number of units in hidden layer 1.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--hidden2',\n",
    "      type=int,\n",
    "      default=32,\n",
    "      help='Number of units in hidden layer 2.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type=int,\n",
    "      default=100,\n",
    "      help='Batch size.  Must divide evenly into the dataset sizes.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--input_data_dir',\n",
    "      type=str,\n",
    "      default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                           'tensorflow/mnist/input_data'),\n",
    "      help='Directory to put the input data.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--log_dir',\n",
    "      type=str,\n",
    "      default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                           'tensorflow/mnist/logs/fully_connected_feed'),\n",
    "      help='Directory to put the log data.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--fake_data',\n",
    "      default=False,\n",
    "      help='If true, uses fake data for unit testing.',\n",
    "      action='store_true'\n",
    "  )\n",
    "\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
